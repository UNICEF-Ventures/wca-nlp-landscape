# Manual benchmark entries for Hausa (hau)
# This file is NEVER overwritten by populate_research.py.
# Add one-off benchmark results you find in papers here.

evaluations:
  asr:
    - model: facebook/mms-1b-fl102
      model_url: https://huggingface.co/facebook/mms-1b-fl102
      results:
        - test_set: FLEURS-102 test
          source: reported
          source_url: https://arxiv.org/abs/2305.13516
          notes: "Table A3 — MMS fine-tuned on FLEURS labeled data, CC LM."
          metrics:
            - name: CER
              value: 5.9
        - test_set: FLEURS-102 dev
          source: reported
          source_url: https://arxiv.org/abs/2305.13516
          notes: "Table A3 — MMS fine-tuned on FLEURS labeled data, CC LM."
          metrics:
            - name: CER
              value: 5.5
    - model: facebook/mms-1b-all
      model_url: https://huggingface.co/facebook/mms-1b-all
      results:
        - test_set: FLEURS test
          source: reported
          source_url: https://arxiv.org/abs/2305.13516
          notes: "Table A1 — MMS 1107-lang model, LSAH + CC LM."
          metrics:
            - name: WER
              value: 26.4
  mt:
    - model: facebook/nllb-200-3.3B
      model_url: https://huggingface.co/facebook/nllb-200-3.3B
      results:
        - test_set: MAFAND-MT
          source: reported
          source_url: https://arxiv.org/abs/2207.04672
          notes: "Table 38 — news domain test set."
          metrics:
            - name: spBLEU
              value: 8.2
            - name: chrF++
              value: 34.8
            - name: direction
              value: "eng→hau"
        - test_set: MAFAND-MT
          source: reported
          source_url: https://arxiv.org/abs/2207.04672
          notes: "Table 38 — news domain test set."
          metrics:
            - name: spBLEU
              value: 13.5
            - name: chrF++
              value: 37.9
            - name: direction
              value: "hau→eng"
    - model: masakhane/m2m100_418M_en_hau_rel_news_ft
      model_url: https://huggingface.co/masakhane/m2m100_418M_en_hau_rel_news_ft
      results:
        - test_set: MAFAND-MT
          source: reported
          source_url: https://aclanthology.org/2022.naacl-main.223/
          notes: "Taken from NLLB paper Table 38 — news domain test set."
          metrics:
            - name: BLEU
              value: 15.9
            - name: chrF++
              value: 42.1
            - name: direction
              value: "eng→hau"
    - model: masakhane/m2m100_418M_hau_en_rel_news_ft
      model_url: https://huggingface.co/masakhane/m2m100_418M_hau_en_rel_news_ft
      results:
        - test_set: MAFAND-MT
          source: reported
          source_url: https://aclanthology.org/2022.naacl-main.223/
          notes: "Taken from NLLB paper Table 38 — news domain test set."
          metrics:
            - name: BLEU
              value: 18.2
            - name: chrF++
              value: 40.2
            - name: direction
              value: "hau→eng"
  llm:
    - model: goldfish-models/hau_latn_full
      model_url: https://huggingface.co/goldfish-models/hau_latn_full
      results:
        - test_set: FLORES-200
          source: reported
          source_url: https://arxiv.org/abs/2408.10441
          notes: "Monolingual LM trained on up to 1GB of hau text. Lower is better."
          metrics:
            - name: Log-perplexity
              value: 86.28
