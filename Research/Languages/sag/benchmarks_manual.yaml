# Manual benchmark entries for Sango (sag)
# This file is NEVER overwritten by populate_research.py.

# Note: Kreyòl-MT scores are from a Bible-dominated test set. Real-world
# performance on general text is likely significantly lower (the paper reports
# a 68→14.7 BLEU drop for Haitian when switching from Bible to Wikipedia test sets).

evaluations:
  translation:
    - model: jhu-clsp/kreyol-mt
      model_url: https://huggingface.co/jhu-clsp/kreyol-mt
      results:
        - test_set: Kreyòl-MT (bible-based)
          source: reported
          source_url: https://arxiv.org/abs/2405.05376
          metrics:
            - name: BLEU (sag→eng)
              value: 39
            - name: BLEU (eng→sag)
              value: 37
            - name: BLEU (sag→fra)
              value: 72
            - name: BLEU (fra→sag)
              value: 90
